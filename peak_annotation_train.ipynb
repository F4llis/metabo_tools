{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 15:06:25.941615: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 15:06:30.035279: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-04 15:06:42.681309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/miniconda3-4.10.3-gpvric5au5ue2cp2qiiar6vijzx4ibnb/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/cudnn-8.4.0.27-11.6-kiy5woy2bxoqxef4wineb5v2qobam5al/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/cuda-11.6.2-hjqfaeelfbajionp4uptpb6grp2uheb6/lib64:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/libxml2-2.9.13-7fb2erckdanb4muwe2vah2al6fzw6jgx/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/zlib-1.2.12-dzl6lpj5rcfbk47kap64ic7azyx237g4/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/xz-5.2.5-ejmeryutjezrbn74anuvrbiu4lck67ou/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/gcc-10.4.0-7fjdev7tbifqfonsjwlqmr6qcnza2ezu/lib64:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/gcc-10.4.0-7fjdev7tbifqfonsjwlqmr6qcnza2ezu/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/mpfr-4.1.0-4owr5jskgrebkppacvwkfa7tdtlqjalg/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/gmp-6.2.1-rg3gv5myy7lt46cjxvmdmsghhyhxguqo/lib\n",
      "2022-10-04 15:06:42.681478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/miniconda3-4.10.3-gpvric5au5ue2cp2qiiar6vijzx4ibnb/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/cudnn-8.4.0.27-11.6-kiy5woy2bxoqxef4wineb5v2qobam5al/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/cuda-11.6.2-hjqfaeelfbajionp4uptpb6grp2uheb6/lib64:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/libxml2-2.9.13-7fb2erckdanb4muwe2vah2al6fzw6jgx/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/zlib-1.2.12-dzl6lpj5rcfbk47kap64ic7azyx237g4/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/xz-5.2.5-ejmeryutjezrbn74anuvrbiu4lck67ou/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/gcc-10.4.0-7fjdev7tbifqfonsjwlqmr6qcnza2ezu/lib64:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen/gcc-8.4.1/gcc-10.4.0-7fjdev7tbifqfonsjwlqmr6qcnza2ezu/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/mpfr-4.1.0-4owr5jskgrebkppacvwkfa7tdtlqjalg/lib:/dcsrsoft/spack/arolle/v1.0/spack/opt/spack/linux-rhel8-zen2/gcc-10.4.0/gmp-6.2.1-rg3gv5myy7lt46cjxvmdmsghhyhxguqo/lib\n",
      "2022-10-04 15:06:42.681485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Input\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "precision = 0.005  # m/z precision for raw data\n",
    "data_per_sec = 2  # spectrum per second\n",
    "half_time_window = 30  # time range before and after RT in sec\n",
    "number_ticks = half_time_window * 2 * data_per_sec  # number of spectrum per data\n",
    "intensity_treshold = 8000 #  everything under this will be annotated false\n",
    "number_sample = 120\n",
    "\n",
    "\n",
    "data_folder = '../data/sara_training/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(data_folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "peak_clarkii1= data_folder + 'clarkii_peaks.csv' # data with peak: mz, rt, Y/N\n",
    "mz_clarkii1 = data_folder + 'clarkii_T0I1.mzML'\n",
    "\n",
    "peak_clarkii10= data_folder + 'AclarkiiT0I10ctrl_peaks.csv' # data with peak: mz, rt, Y/N\n",
    "mz_clarkii10 = data_folder + 'AclarkiiT0I10ctrl.mzML'\n",
    "\n",
    "peak_viridis= data_folder + 'viridis_BEHamide_peaks.csv' # data with peak: mz, rt, Y/N\n",
    "mz_viridis = data_folder + 'viridisT0I2.mzML'\n",
    "\n",
    "pickle_path = './pickle_training.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_np = False\n",
    "if save_np == True:\n",
    "\n",
    "    data_clark1 = util.build_data_ml(peak_clarkii1,mz_clarkii1 )\n",
    "    data_clark10 = util.build_data_ml(peak_clarkii10,mz_clarkii10, mz_rt_sec=False)\n",
    "    data_viri = util.build_data_ml(peak_viridis,mz_viridis )\n",
    "\n",
    "    all_data = [None,None,None,None]\n",
    "    all_data[0] = data_clark1[0] + data_clark10[0] + data_viri[0]\n",
    "    all_data[1] = data_clark1[1] + data_clark10[1] +  data_viri[1]\n",
    "    all_data[2] = data_clark1[2] + data_clark10[2] + data_viri[2]\n",
    "    all_data[3] = data_clark1[3] + data_clark10[3] + data_viri[3]\n",
    "\n",
    "    with open(pickle_path, 'wb') as fi:\n",
    "        np.save(fi, np.asanyarray(data_clark10, dtype=object))\n",
    "    data = np.load(pickle_path, allow_pickle=True)\n",
    "\n",
    "else:\n",
    "    data = np.load(pickle_path, allow_pickle=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_ = data[0]\n",
    "y_ =  data[1]\n",
    "mz_ = data[2]\n",
    "rt_ =  data[3]\n",
    "\n",
    "import random\n",
    "for i in random.sample(range(0,len(X_)),20):\n",
    "    color = 'green' if y_[i] == 1 else 'red'\n",
    "    yn = 'YES' if y_[i] == 1 else 'NO'\n",
    "    plt.title(  str(i) + ' ' + yn + ' - ' + str(mz_[i]) + ' // ' + str(float(rt_[i])))\n",
    "    plt.plot(X_[i], color=color)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "\n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal\n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "\n",
    "    input:\n",
    "        x: the input signal\n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "\n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "\n",
    "    see also:\n",
    "\n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    "\n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "\n",
    "def NormalizeData(data):\n",
    "    if np.max(data) - np.min(data) == 0.0:\n",
    "        return data\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "\n",
    "def getStat(data):\n",
    "    s = [0, 0, 0, 0, 0]  # nbrObs, mean, var,skew, kurtosis\n",
    "\n",
    "    if len(data) > 0:\n",
    "        de = scipy.stats.describe(data)\n",
    "        s = [de[0]/number_sample, de[2], de[3], de[4], de[5]]\n",
    "\n",
    "    return np.nan_to_num(np.array(s))\n",
    "\n",
    "\n",
    "def filter_threshold(values):\n",
    "    if max(values) >= intensity_treshold:\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_)\n",
    "\n",
    "df['y']= y_\n",
    "df['mz']= mz_\n",
    "df['rt']= rt_\n",
    "\n",
    "\n",
    "df = df[df[0].map(lambda  x: filter_threshold(x))]\n",
    "\n",
    "df['y'] = df['y'].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "df['norm'] = df[0].apply(lambda x: NormalizeData(x) )\n",
    "\n",
    "df['smooth'] = df['norm'].apply(lambda x: smooth(x) )\n",
    "df['smooth'] = df['smooth'].apply(lambda x: x[5:-5] )\n",
    "\n",
    "df['grad1'] = df['smooth'].apply(lambda x: np.gradient(x) )\n",
    "df['grad2'] = df['grad1'].apply(lambda x: np.gradient(x) )\n",
    "\n",
    "\n",
    "# Maxima & Minima\n",
    "df['maxima'] = df['smooth'].apply(lambda x: scipy.signal.find_peaks(x, width=None, wlen=None, rel_height=0.5, plateau_size=None)[0] )\n",
    "df['minima'] = df['smooth'].apply(lambda x: scipy.signal.find_peaks(-x, height=None, threshold=None, distance=None, prominence=None, width=None, wlen=None, rel_height=0.5, plateau_size=None)[0] )\n",
    "\n",
    "df['maxval'] =  df[['smooth', 'maxima']].apply(lambda x: [x.smooth[i] for i in x.maxima], axis=1)\n",
    "df['minval'] =  df[['smooth', 'minima']].apply(lambda x: [x.smooth[i] for i in x.minima], axis=1)\n",
    "\n",
    "df['maxvalnorm'] =  df['maxval'].apply(lambda x: NormalizeData(x) if len(x) > 0 else [] )\n",
    "df['minvalnorm'] =  df['minval'].apply(lambda x: NormalizeData(x) if len(x) > 0 else [] )\n",
    "\n",
    "# Stats\n",
    "df['maxvalstats'] =  df['maxvalnorm'].apply(lambda x: getStat(x) )\n",
    "df['minvalstats'] =  df['minvalnorm'].apply(lambda x: getStat(x) )\n",
    "\n",
    "df['maximastats'] =  df['maxima'].apply(lambda x: getStat(x) )\n",
    "df['minimastats'] =  df['minima'].apply(lambda x: getStat(x) )\n",
    "\n",
    "df['stats'] =  df['smooth'].apply(lambda x: getStat(x) )\n",
    "\n",
    "df['fft'] = df[0].apply(lambda x: np.fft.fft(x) )\n",
    "df['fftr'] = df['fft'].apply(lambda x: np.real(x) )\n",
    "df['ffti'] = df['fft'].apply(lambda x: np.real(np.imag(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ret_mats(df):\n",
    "    xfft = np.hstack([np.vstack(df['fftr']), np.vstack(df['ffti']),\n",
    "                      np.vstack(df['maxvalstats']),\n",
    "                  np.vstack(df['minvalstats']),\n",
    "                  np.vstack(df['maximastats']),\n",
    "                  np.vstack(df['minimastats']),\n",
    "                  np.vstack(df['stats'])\n",
    "                  ])\n",
    "\n",
    "    xstat = np.hstack([np.vstack(df['maxvalstats']),\n",
    "                       np.vstack(df['minvalstats']),\n",
    "                  np.vstack(df['maximastats']),\n",
    "                  np.vstack(df['minimastats']),\n",
    "                  np.vstack(df['stats'])\n",
    "                  ])\n",
    "\n",
    "    x = np.stack([ np.vstack(df['norm']), np.vstack(df['smooth']) ,\n",
    "              np.vstack(df['grad1']) , np.vstack(df['grad2'])  ] , axis = 2)\n",
    "    y = df.y.map(lambda x : float(x))\n",
    "\n",
    "    return x,xfft,xstat, y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.15)\n",
    "\n",
    "mats={'train':ret_mats(train), 'test':ret_mats(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "inputvec = Input(shape=(120,4))\n",
    "inputfft = Input(shape=(265))\n",
    "inputstat = Input(shape=(25))\n",
    "\n",
    "lstm = LSTM(300,  return_sequences = True , return_state = False, dropout=0.2,\n",
    "    recurrent_dropout=0.1, go_backwards=True)\n",
    "\n",
    "whole_seq_output = lstm(inputvec)\n",
    "\n",
    "lstm2 = LSTM(200, return_sequences = True , return_state = False, dropout=0.2,\n",
    "    recurrent_dropout=0.1, go_backwards=True )\n",
    "\n",
    "whole_seq_output2 = lstm2(whole_seq_output)\n",
    "\n",
    "lstm3 = LSTM(100, return_sequences = False , return_state = True, dropout=0.2,\n",
    "    recurrent_dropout=0.1, go_backwards=True )\n",
    "\n",
    "\n",
    "final_memory_state, final_carry_state, whole_seq_output = lstm3(whole_seq_output2)\n",
    "\n",
    "\n",
    "aux_input = tf.keras.layers.Concatenate()([final_memory_state, final_carry_state])\n",
    "output1 = Dense(100,activation = 'tanh')(aux_input)\n",
    "aux = Dense(1,activation = 'tanh' , name = 'AuxRnnOut' )(output1)\n",
    "\n",
    "\n",
    "#dense_input = tf.keras.layers.Concatenate()([final_memory_state, final_carry_state,whole_seq_output, inputfft])\n",
    "\n",
    "dense_input = tf.keras.layers.Concatenate()([final_memory_state, final_carry_state, inputfft,inputstat])\n",
    "dense_input = tf.keras.layers.Dropout(0.15)(dense_input)\n",
    "\n",
    "output1 = Dense(300,activation = 'tanh')(dense_input)\n",
    "output1 = tf.keras.layers.Dropout(0.15)(output1)\n",
    "\n",
    "#dense_input = tf.keras.layers.Concatenate()([output1,inputstat ])\n",
    "output1 = Dense(100,activation = 'tanh')(output1)\n",
    "output = tf.keras.layers.Dropout(0.1)(output1)\n",
    "\n",
    "final = Dense(1,activation = 'tanh' , name = 'GlobalOut' )(output)\n",
    "\n",
    "\n",
    "model = Model( inputs = [inputvec,inputfft,inputstat ] , outputs = [final,aux])\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam( learning_rate=0.005)\n",
    "#opt = tf.keras.optimizers.RMSprop( learning_rate=0.005)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit([mats['train'][0], mats['train'][1], mats['train'][2]], [ mats['train'][3],mats['train'][3]] , shuffle = True, epochs=400, batch_size=300 , verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate([ mats['test'][0], mats['test'][1], mats['test'][2]], mats['test'][3], verbose=1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('output/model_peak_' + str(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pre_raw = model.predict([ mats['test'][0], mats['test'][1], mats['test'][2]])\n",
    "\n",
    "pre = [1 if  pr > 0.5 else 0 for pr in pre_raw ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "cpt =0\n",
    "\n",
    "for y_pred in pre:\n",
    "\n",
    "    y_ref = list(mats['test'][3])[cpt]\n",
    "    intensity = list(test[0])[cpt]\n",
    "\n",
    "    if int(y_pred) != int(y_ref):\n",
    "\n",
    "        if y_pred == 1:\n",
    "            fp += 1\n",
    "            plt.title( 'FP ' + str(list(test['mz'])[cpt]) + ' / '+ str(list(test['rt'])[cpt]) )\n",
    "        else:\n",
    "            fn += 1\n",
    "            plt.title( 'FN ' + str(list(test['mz'])[cpt]) + ' / '+ str(list(test['rt'])[cpt]) )\n",
    "\n",
    "\n",
    "        color = 'green' if y_pred == 1 else 'red'\n",
    "        plt.plot(intensity , color = color)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        if y_pred == 1:\n",
    "            tp +=1\n",
    "            plt.title( 'TP ' + str(list(test['mz'])[cpt]) + ' / '+ str(list(test['rt'])[cpt]) )\n",
    "        else:\n",
    "            tn +=1\n",
    "            plt.title( 'TN ' + str(list(test['mz'])[cpt]) + ' / '+ str(list(test['rt'])[cpt]) )\n",
    "\n",
    "\n",
    "\n",
    "        color = 'green' if y_pred == 1 else 'red'\n",
    "        plt.plot(intensity , color = color)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    cpt +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total = len( mats['test'][0])\n",
    "false =  tn + fp\n",
    "print('Predicted Features: ', total)\n",
    "print('False Predictions: ', false)\n",
    "print('% Good: ', (total-false)/total*100)\n",
    "print('\\n')\n",
    "\n",
    "print('TP', tp)\n",
    "print('TN', tn)\n",
    "print('FP', fp)\n",
    "print('FN', fn)\n",
    "print('\\n')\n",
    "\n",
    "print('Precision: ', (100 * tp)/ (tp+fp) )\n",
    "print('Recall: ', (100 * tp/ (tp+fn) ))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
